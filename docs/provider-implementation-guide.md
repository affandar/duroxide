# Provider Implementation Guide

**For:** LLMs and humans implementing new Duroxide providers  
**Reference:** See `src/providers/sqlite.rs` for complete working implementation

---

## Quick Start

To implement a Provider for Duroxide, you need to:

1. Implement the `Provider` trait from `duroxide::providers`
2. Store orchestration history (append-only event log)
3. Manage 3 work queues with peek-lock semantics
4. Ensure atomic commits for orchestration turns

**Key Principle:** Providers are storage abstractions. The runtime owns orchestration logic.

## ‚ö†Ô∏è CRITICAL: ID Generation Contract

**The provider MUST NOT generate `execution_id` or `event_id` values.**

All IDs are generated by the runtime and passed to the provider:

- **`execution_id`**: Passed explicitly to `ack_orchestration_item()`
- **`event_id`**: Set in each `Event` within the `history_delta`

**The provider's role is to STORE these IDs, not generate them.**

### ‚úÖ Correct Pattern
```rust
async fn ack_orchestration_item(
    execution_id: u64,        // ‚úÖ Runtime provides this
    history_delta: Vec<Event>, // ‚úÖ Each event has event_id set by runtime
) {
    // Validate runtime provided IDs
    for event in &history_delta {
        assert!(event.event_id() > 0, "event_id must be set by runtime");
    }
    
    // Store execution with runtime-provided ID
    db.insert_execution(instance, execution_id, "Running");
    
    // Store events with runtime-provided event_ids
    for event in history_delta {
        db.insert_event(instance, execution_id, event.event_id(), event);
    }
}
```

### ‚ùå Wrong Pattern (DO NOT DO THIS)
```rust
// WRONG - Provider generating execution_id
let execution_id = db.query("SELECT MAX(execution_id) + 1 FROM executions")?;

// WRONG - Provider generating event_id
let event_id = db.query("SELECT MAX(event_id) + 1 FROM history")?;
for mut event in history_delta {
    event.set_event_id(event_id);  // ‚ùå Never modify event_id!
    event_id += 1;
}
```

---

## Complete Implementation Template

```rust
use duroxide::providers::{Provider, WorkItem, OrchestrationItem, ExecutionMetadata};
use duroxide::Event;
use std::sync::Arc;

pub struct MyProvider {
    // Your storage client (e.g., connection pool)
    // Lock timeout (recommended: 30 seconds)
    // Any configuration
}

#[async_trait::async_trait]
impl Provider for MyProvider {
    // === REQUIRED: Core Orchestration Methods ===
    
    async fn fetch_orchestration_item(&self) -> Option<OrchestrationItem> {
        // 1. Find next available message in orchestrator queue
        // 2. Lock ALL messages for that instance
        // 3. Load instance metadata (name, version, execution_id)
        // 4. Load history for current execution_id
        // 5. Return OrchestrationItem with unique lock_token
        
        todo!("See detailed docs below")
    }
    
    async fn ack_orchestration_item(
        &self,
        lock_token: &str,
        execution_id: u64,
        history_delta: Vec<Event>,
        worker_items: Vec<WorkItem>,
        orchestrator_items: Vec<WorkItem>,
        metadata: ExecutionMetadata,
    ) -> Result<(), String> {
        // ALL operations must be atomic (single transaction):
        // 1. Idempotently create execution row for the explicit execution_id (INSERT OR IGNORE)
        // 2. Update instances.current_execution_id = MAX(current_execution_id, execution_id)
        // 3. Append history_delta to the event log for that execution_id
        // 4. Update execution status/output from metadata (DON'T inspect events!)
        // 5. Enqueue worker_items to worker queue
        // 6. Enqueue orchestrator_items to orchestrator queue (may include TimerFired with visible_at delay)
        // 7. Delete locked messages (release lock)
        
        todo!("See detailed docs below")
    }
    
    async fn abandon_orchestration_item(&self, lock_token: &str, delay_ms: Option<u64>) -> Result<(), String> {
        // Clear lock_token from messages
        // Optionally delay visibility for backoff
        
        todo!("See detailed docs below")
    }
    
    // === REQUIRED: History Access ===
    
    async fn read(&self, instance: &str) -> Vec<Event> {
        // Return events for LATEST execution, ordered by event_id
        // Return empty Vec if instance doesn't exist
        
        todo!()
    }
    
    async fn append_with_execution(
        &self,
        instance: &str,
        execution_id: u64,
        new_events: Vec<Event>,
    ) -> Result<(), String> {
        // Append events to history for specified execution
        // DO NOT modify event_ids (runtime assigns these)
        // Reject or ignore duplicates (same event_id)
        
        todo!()
    }
    
    // === REQUIRED: Worker Queue ===
    
    async fn enqueue_worker_work(&self, item: WorkItem) -> Result<(), String> {
        // Add item to worker queue
        // New items should have lock_token = NULL
        
        todo!()
    }
    
    async fn dequeue_worker_peek_lock(&self) -> Option<(WorkItem, String)> {
        // Find next unlocked item
        // Lock it with unique token
        // Return item + token (item stays in queue)
        
        todo!()
    }
    
    async fn ack_worker(&self, token: &str, completion: WorkItem) -> Result<(), String> {
        // Atomically:
        // 1. Delete item from worker queue (WHERE lock_token = token)
        // 2. Enqueue completion (ActivityCompleted or ActivityFailed) to orchestrator queue
        // MUST be atomic - prevents lost completions or duplicate work
        
        todo!()
    }
    
    // === REQUIRED: Orchestrator Queue ===
    
    async fn enqueue_orchestrator_work(&self, item: WorkItem, delay_ms: Option<u64>) -> Result<(), String> {
        // Extract instance from item (see WorkItem docs)
        // Set visible_at = now() + delay_ms.unwrap_or(0)
        // For StartOrchestration, create instance+execution rows
        
        todo!()
    }
    
    // === OPTIONAL: Multi-Execution Support ===
    
    async fn latest_execution_id(&self, instance: &str) -> Option<u64> {
        // Return MAX(execution_id) for instance. Override for performance.
        None
    }
    
    async fn read_with_execution(&self, instance: &str, _execution_id: u64) -> Vec<Event> {
        // Return events for specific execution_id (not just latest). Used for debugging/testing.
        // Default can delegate to read(); production providers should override.
        self.read(instance).await
    }
    
    // === OPTIONAL: Management APIs ===
    
    async fn list_instances(&self) -> Vec<String> {
        Vec::new()  // Override if you want instance listing
    }
    
    async fn list_executions(&self, instance: &str) -> Vec<u64> {
        let h = self.read(instance).await;
        if h.is_empty() { Vec::new() } else { vec![1] }
    }
}
```

---

## Detailed Implementation Guide

### 1. fetch_orchestration_item() - The Orchestration Turn Fetcher

**Purpose:** Atomically fetch and lock work for one instance.

**Pseudo-code:**
```
BEGIN TRANSACTION

// Find next work (visibility + lock filtering)
row = SELECT id, instance_id FROM orchestrator_queue
      WHERE visible_at <= current_timestamp()
        AND (lock_token IS NULL OR locked_until <= current_timestamp())
      ORDER BY id ASC
      LIMIT 1

IF row IS NULL:
    ROLLBACK
    RETURN None

instance_id = row.instance_id

// Lock ALL messages for this instance
lock_token = generate_uuid()
locked_until = current_timestamp() + lock_timeout_ms

UPDATE orchestrator_queue
SET lock_token = lock_token, locked_until = locked_until
WHERE instance_id = instance_id
  AND (lock_token IS NULL OR locked_until <= current_timestamp())

// Fetch all locked messages
messages = SELECT work_item FROM orchestrator_queue
           WHERE lock_token = lock_token
           ORDER BY id ASC

messages = messages.map(|m| deserialize_workitem(m))

// Load instance metadata
metadata = SELECT orchestration_name, orchestration_version, current_execution_id
           FROM instances
           WHERE instance_id = instance_id

IF metadata IS NULL:
    // New instance - derive from first message or history
    IF messages.first() IS StartOrchestration:
        orchestration_name = messages.first().orchestration
        orchestration_version = messages.first().version.unwrap_or("1.0.0")
        current_execution_id = 1
    ELSE:
        // Shouldn't happen - check history as fallback
        RETURN None or build from history

orchestration_name = metadata.orchestration_name
orchestration_version = metadata.orchestration_version  
current_execution_id = metadata.current_execution_id

// Load history for current execution
history_rows = SELECT event_data FROM history
               WHERE instance_id = instance_id
                 AND execution_id = current_execution_id
               ORDER BY event_id ASC

history = history_rows.map(|row| deserialize_event(row.event_data))

COMMIT TRANSACTION

RETURN Some(OrchestrationItem {
    instance: instance_id,
    orchestration_name,
    execution_id: current_execution_id,
    version: orchestration_version,
    history,
    messages,
    lock_token,
})
```

**Edge Cases:**
- No work available ‚Üí Return None (dispatcher will sleep)
- Lock contention ‚Üí Transaction retry or return None
- Missing instance metadata ‚Üí Derive from messages or history
- Empty history ‚Üí Valid (new instance)

---

### 2. ack_orchestration_item() - The Atomic Commit

**Purpose:** Atomically commit all changes from an orchestration turn.

**Pseudo-code:**
```
BEGIN TRANSACTION

// Get instance_id from lock_token
instance_id = SELECT DISTINCT instance_id FROM orchestrator_queue
              WHERE lock_token = lock_token

IF instance_id IS NULL:
    ROLLBACK
    RETURN Err("Invalid lock token")

// Use the explicit execution_id provided by the runtime

// 1. Idempotently create execution row and update instance pointer
INSERT INTO executions (instance_id, execution_id, status, started_at)
VALUES (instance_id, execution_id, 'Running', CURRENT_TIMESTAMP)
ON CONFLICT DO NOTHING;

UPDATE instances
SET current_execution_id = MAX(current_execution_id, execution_id)
WHERE instance_id = instance_id;

// 2. Append history_delta (DO NOT inspect events!)
FOR event IN history_delta:
    // Validate event_id was set by runtime
    IF event.event_id() == 0:
        RETURN Err("event_id must be set by runtime")
    
    event_json = serialize(event)
    event_type = extract_discriminant_name(event)  // For indexing only
    
    INSERT INTO history (instance_id, execution_id, event_id, event_type, event_data, created_at)
    VALUES (instance_id, execution_id, event.event_id(), event_type, event_json, NOW())
    ON CONFLICT DO NOTHING  // Idempotent

// 3. Update execution metadata (NO event inspection!)
IF metadata.status IS NOT NULL:
    UPDATE executions
    SET status = metadata.status,
        output = metadata.output,
        completed_at = CURRENT_TIMESTAMP
    WHERE instance_id = instance_id
      AND execution_id = execution_id

// (No implicit next-execution creation here; runtime controls execution_id explicitly)

// 4. Enqueue worker items
FOR item IN worker_items:
    work_json = serialize(item)
    INSERT INTO worker_queue (work_item, lock_token, locked_until, created_at)
    VALUES (work_json, NULL, NULL, NOW())

// 5. Enqueue orchestrator items (may include TimerFired with delayed visibility)
FOR item IN orchestrator_items:
    work_json = serialize(item)
    target_instance = extract_instance(item)  // See WorkItem docs
    
    // Set visible_at based on item type
    visible_at = IF item IS TimerFired:
                     item.fire_at_ms  // Delayed visibility for timers
                 ELSE:
                     NOW()  // Immediate visibility for other items
    
    // Special case: StartOrchestration needs instance creation
    IF item IS StartOrchestration:
        INSERT INTO instances (instance_id, orchestration_name, orchestration_version, current_execution_id)
        VALUES (target_instance, item.orchestration, item.version.unwrap_or("1.0.0"), 1)
        ON CONFLICT DO NOTHING
        
        INSERT INTO executions (instance_id, execution_id, status)
        VALUES (target_instance, 1, 'Running')
        ON CONFLICT DO NOTHING
    
    INSERT INTO orchestrator_queue (instance_id, work_item, visible_at, lock_token, locked_until, created_at)
    VALUES (target_instance, work_json, visible_at, NULL, NULL, NOW())

// 6. Release lock (delete acknowledged messages)
DELETE FROM orchestrator_queue WHERE lock_token = lock_token

COMMIT TRANSACTION
RETURN Ok(())
```

**Critical:**
- All 6 steps must be in ONE transaction
- If ANY step fails, ROLLBACK everything
- Never partially commit

---

### 3. abandon_orchestration_item() - Release Lock for Retry

**Pseudo-code:**
```
visible_at = IF delay_ms IS NOT NULL:
                 current_timestamp() + delay_ms
             ELSE:
                 current_timestamp()

UPDATE orchestrator_queue
SET lock_token = NULL,
    locked_until = NULL,
    visible_at = visible_at
WHERE lock_token = lock_token

// Idempotent - Ok if no rows updated
RETURN Ok(())
```

---

### 4. read() - Load Latest Execution History

**Pseudo-code:**
```
// Get latest execution ID
execution_id = SELECT COALESCE(MAX(execution_id), 1)
               FROM executions
               WHERE instance_id = instance

// Load events for that execution
rows = SELECT event_data FROM history
       WHERE instance_id = instance
         AND execution_id = execution_id
       ORDER BY event_id ASC

events = rows.map(|row| deserialize_event(row.event_data))
         .collect()

RETURN events  // Empty Vec if instance doesn't exist
```

---

### 5. Worker Queue Operations

**enqueue_worker_work():**
```
work_json = serialize(item)
INSERT INTO worker_queue (work_item, lock_token, locked_until)
VALUES (work_json, NULL, NULL)
```

**dequeue_worker_peek_lock():**
```
BEGIN TRANSACTION

row = SELECT id, work_item FROM worker_queue
      WHERE lock_token IS NULL OR locked_until <= current_timestamp()
      ORDER BY id ASC
      LIMIT 1

IF row IS NULL:
    ROLLBACK
    RETURN None

lock_token = generate_uuid()
locked_until = current_timestamp() + lock_timeout_ms

UPDATE worker_queue
SET lock_token = lock_token, locked_until = locked_until
WHERE id = row.id

COMMIT
item = deserialize_workitem(row.work_item)
RETURN Some((item, lock_token))
```

**ack_worker():**
```
DELETE FROM worker_queue WHERE lock_token = lock_token
RETURN Ok(())  // Idempotent
```

---

## Schema Recommendations

### Minimum Required Tables

1. **history** - Append-only event log
   - PRIMARY KEY: (instance_id, execution_id, event_id)
   - Columns: event_data (JSON), event_type (for indexing), created_at

2. **orchestrator_queue** - Orchestration work items
   - PRIMARY KEY: id (auto-increment)
   - Columns: instance_id, work_item (JSON), visible_at, lock_token, locked_until
   - INDEX: (visible_at, lock_token) for fetch performance

3. **worker_queue** - Activity execution requests
   - PRIMARY KEY: id (auto-increment)
   - Columns: work_item (JSON), lock_token, locked_until

4. **instances** - Instance metadata
   - PRIMARY KEY: instance_id
   - Columns: orchestration_name, orchestration_version, current_execution_id

5. **executions** - Execution tracking
   - PRIMARY KEY: (instance_id, execution_id)
   - Columns: status, output, started_at, completed_at

---


## Common Pitfalls

### ‚ùå DON'T: Inspect Event Contents

```rust
// WRONG - Provider inspecting events
for event in &history_delta {
    match event {
        Event::OrchestrationCompleted { output, .. } => {
            // Provider understanding orchestration semantics
            self.update_status("Completed", output)?;
        }
    }
}
```

```rust
// CORRECT - Use metadata from runtime
if let Some(status) = &metadata.status {
    self.update_status(status, &metadata.output)?;
}
```

### ‚ùå DON'T: Modify Event IDs

```rust
// WRONG - Renumbering events
let mut next_id = self.get_next_event_id()?;
for mut event in history_delta {
    event.set_event_id(next_id);  // DON'T DO THIS!
    next_id += 1;
}
```

```rust
// CORRECT - Store as-is
for event in &history_delta {
    assert!(event.event_id() > 0, "Runtime must set event_ids");
    self.insert_event(event)?;  // Use runtime-assigned IDs
}
```

### ‚ùå DON'T: Break Atomicity

```rust
// WRONG - Non-atomic
self.append_history(history_delta).await?;  // Committed!
self.enqueue_workers(worker_items).await?;  // If this fails, history already saved!
self.release_lock(lock_token).await?;
```

```rust
// CORRECT - Single transaction
let tx = self.begin_transaction().await?;
self.append_history_in_tx(&tx, history_delta).await?;
self.enqueue_workers_in_tx(&tx, worker_items).await?;
self.release_lock_in_tx(&tx, lock_token).await?;
tx.commit().await?;  // All or nothing
```

---

## Testing Your Provider

Use `tests/sqlite_provider_test.rs` as a template. Key tests:

1. **Basic workflow** (test_sqlite_provider_basic)
   - Enqueue StartOrchestration
   - Fetch and verify OrchestrationItem
   - Ack with history
   - Verify history saved

2. **Atomicity** (test_sqlite_provider_transactional)
   - Ack with multiple operations
   - Verify all succeeded or all failed

3. **Lock expiration** (test_lock_expiration)
   - Fetch but don't ack
   - Wait for lock timeout
   - Verify message redelivered

4. **Execution status persistence** (test_execution_status_*)
   - Verify metadata properly stored
   - Test Completed, Failed, ContinuedAsNew states

5. **Multi-execution** (test_execution_status_continued_as_new)
   - ContinueAsNew is runtime-owned; provider persists explicit execution_id on ack
   - Verify `current_execution_id` and statuses updated via metadata

---

## Performance Considerations

### Indexes (Critical for Performance)

```sql
-- Orchestrator queue (hot path)
CREATE INDEX idx_orch_visible ON orchestrator_queue(visible_at, lock_token);
CREATE INDEX idx_orch_instance ON orchestrator_queue(instance_id);

-- Worker queue
CREATE INDEX idx_worker_lock ON worker_queue(lock_token);

-- History (for read operations)
CREATE INDEX idx_history_lookup ON history(instance_id, execution_id, event_id);
```

### Connection Pooling

- Use connection pools for concurrent dispatcher access
- Recommended pool size: 5-10 connections
- SQLite example: `SqlitePoolOptions::new().max_connections(10)`

### Lock Timeout

- Recommended: 30 seconds
- Too short: False retries under load
- Too long: Slow recovery from crashes

---

## Validation Checklist

Before considering your provider production-ready:

- [ ] All 12 provider tests from sqlite_provider_test.rs pass
- [ ] fetch_orchestration_item returns None when queue empty
- [ ] ack_orchestration_item is fully atomic (single transaction)
- [ ] Lock expiration works (messages redelivered after timeout)
- [ ] Multi-execution support (ContinueAsNew creates execution 2, 3, ...)
- [ ] Execution metadata stored correctly (status, output)
- [ ] History ordering preserved (events returned in event_id order)
- [ ] Concurrent access safe (run with RUST_TEST_THREADS=10)
- [ ] No event content inspection (use ExecutionMetadata only)
- [ ] Worker queue FIFO behavior
- [ ] No duplicate event IDs (PRIMARY KEY enforced)

---

## Example Providers to Implement

### Easy
- **In-Memory Provider**: Use HashMap + RwLock
- **File-based Provider**: JSON files + file locks

### Medium
- **PostgreSQL Provider**: Similar to SQLite, better concurrency
- **MySQL Provider**: Similar to PostgreSQL

### Advanced
- **Redis Provider**: Lua scripts for atomicity, sorted sets for history
- **DynamoDB Provider**: Conditional writes, GSI for queues
- **Cosmos DB Provider**: Change feed for queues, partition by instance_id

---

## Implementing ManagementCapability (Optional)

The `ManagementCapability` trait provides rich management and observability features for your provider. While optional, implementing it enables powerful introspection and monitoring capabilities.

### When to Implement

**Implement ManagementCapability if:**
- Your provider is for production use
- You need instance/execution discovery
- You want system metrics and monitoring
- You need debugging/troubleshooting capabilities

**Skip if:**
- Building a minimal/test provider
- Storage backend doesn't support efficient queries
- Only need core orchestration execution

### The Trait

```rust
#[async_trait::async_trait]
pub trait ManagementCapability: Send + Sync {
    // Discovery
    async fn list_instances(&self) -> Result<Vec<String>, String>;
    async fn list_instances_by_status(&self, status: &str) -> Result<Vec<String>, String>;
    async fn list_executions(&self, instance: &str) -> Result<Vec<u64>, String>;
    
    // History access
    async fn read_execution(&self, instance: &str, execution_id: u64) -> Result<Vec<Event>, String>;
    async fn latest_execution_id(&self, instance: &str) -> Result<u64, String>;
    
    // Metadata
    async fn get_instance_info(&self, instance: &str) -> Result<InstanceInfo, String>;
    async fn get_execution_info(&self, instance: &str, execution_id: u64) -> Result<ExecutionInfo, String>;
    
    // System metrics
    async fn get_system_metrics(&self) -> Result<SystemMetrics, String>;
    async fn get_queue_depths(&self) -> Result<QueueDepths, String>;
}
```

### Implementing the Trait

```rust
use duroxide::providers::{Provider, ManagementCapability, InstanceInfo, ExecutionInfo, SystemMetrics, QueueDepths};

#[async_trait::async_trait]
impl Provider for MyProvider {
    // ... core Provider methods ...
    
    fn as_management_capability(&self) -> Option<&dyn ManagementCapability> {
        Some(self)  // Enable management features
    }
}

#[async_trait::async_trait]
impl ManagementCapability for MyProvider {
    async fn list_instances(&self) -> Result<Vec<String>, String> {
        // Return all instance IDs
        let rows = sqlx::query!("SELECT instance_id FROM instances ORDER BY created_at")
            .fetch_all(&self.pool)
            .await
            .map_err(|e| format!("Failed to list instances: {}", e))?;
        
        Ok(rows.into_iter().map(|r| r.instance_id).collect())
    }
    
    async fn list_instances_by_status(&self, status: &str) -> Result<Vec<String>, String> {
        // Return instances with specific status (Running, Completed, Failed)
        let rows = sqlx::query!(
            "SELECT DISTINCT i.instance_id 
             FROM instances i
             JOIN executions e ON i.instance_id = e.instance_id 
               AND i.current_execution_id = e.execution_id
             WHERE e.status = ?
             ORDER BY i.created_at",
            status
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| format!("Failed to list instances by status: {}", e))?;
        
        Ok(rows.into_iter().map(|r| r.instance_id).collect())
    }
    
    async fn list_executions(&self, instance: &str) -> Result<Vec<u64>, String> {
        // Return all execution IDs for an instance
        let rows = sqlx::query!(
            "SELECT execution_id FROM executions 
             WHERE instance_id = ? 
             ORDER BY execution_id",
            instance
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| format!("Failed to list executions: {}", e))?;
        
        Ok(rows.into_iter().map(|r| r.execution_id as u64).collect())
    }
    
    async fn read_execution(&self, instance: &str, execution_id: u64) -> Result<Vec<Event>, String> {
        // Return history for specific execution (not just latest)
        let rows = sqlx::query!(
            "SELECT event_data FROM history 
             WHERE instance_id = ? AND execution_id = ? 
             ORDER BY event_id",
            instance,
            execution_id as i64
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| format!("Failed to read execution: {}", e))?;
        
        rows.into_iter()
            .filter_map(|row| serde_json::from_str(&row.event_data).ok())
            .collect::<Result<Vec<_>, _>>()
            .map_err(|e| format!("Failed to deserialize events: {}", e))
    }
    
    async fn latest_execution_id(&self, instance: &str) -> Result<u64, String> {
        // Return highest execution ID
        let row = sqlx::query!(
            "SELECT current_execution_id FROM instances WHERE instance_id = ?",
            instance
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| format!("Failed to get latest execution: {}", e))?;
        
        row.map(|r| r.current_execution_id as u64)
            .ok_or_else(|| "Instance not found".to_string())
    }
    
    async fn get_instance_info(&self, instance: &str) -> Result<InstanceInfo, String> {
        // Return instance metadata
        let row = sqlx::query!(
            "SELECT i.orchestration_name, i.orchestration_version, 
                    i.current_execution_id, i.created_at,
                    e.status, e.output
             FROM instances i
             LEFT JOIN executions e ON i.instance_id = e.instance_id 
               AND i.current_execution_id = e.execution_id
             WHERE i.instance_id = ?",
            instance
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| format!("Failed to get instance info: {}", e))?
        .ok_or_else(|| "Instance not found".to_string())?;
        
        Ok(InstanceInfo {
            instance_id: instance.to_string(),
            orchestration_name: row.orchestration_name,
            orchestration_version: row.orchestration_version,
            current_execution_id: row.current_execution_id as u64,
            status: row.status.unwrap_or_else(|| "Running".to_string()),
            output: row.output,
            created_at: row.created_at,
        })
    }
    
    async fn get_execution_info(&self, instance: &str, execution_id: u64) -> Result<ExecutionInfo, String> {
        // Return execution-specific metadata
        let row = sqlx::query!(
            "SELECT status, output, started_at, completed_at 
             FROM executions 
             WHERE instance_id = ? AND execution_id = ?",
            instance,
            execution_id as i64
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| format!("Failed to get execution info: {}", e))?
        .ok_or_else(|| "Execution not found".to_string())?;
        
        Ok(ExecutionInfo {
            instance_id: instance.to_string(),
            execution_id,
            status: row.status,
            output: row.output,
            started_at: row.started_at,
            completed_at: row.completed_at,
        })
    }
    
    async fn get_system_metrics(&self) -> Result<SystemMetrics, String> {
        // Return system-wide statistics
        let total_instances = sqlx::query!("SELECT COUNT(*) as count FROM instances")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        let total_executions = sqlx::query!("SELECT COUNT(*) as count FROM executions")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        let running = sqlx::query!("SELECT COUNT(*) as count FROM executions WHERE status = 'Running'")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        let completed = sqlx::query!("SELECT COUNT(*) as count FROM executions WHERE status = 'Completed'")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        let failed = sqlx::query!("SELECT COUNT(*) as count FROM executions WHERE status = 'Failed'")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        let total_events = sqlx::query!("SELECT COUNT(*) as count FROM history")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get metrics: {}", e))?
            .count as usize;
        
        Ok(SystemMetrics {
            total_instances,
            total_executions,
            running_instances: running,
            completed_instances: completed,
            failed_instances: failed,
            total_events,
        })
    }
    
    async fn get_queue_depths(&self) -> Result<QueueDepths, String> {
        // Return current queue depths
        let orch = sqlx::query!("SELECT COUNT(*) as count FROM orchestrator_queue")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get queue depths: {}", e))?
            .count as usize;
        
        let worker = sqlx::query!("SELECT COUNT(*) as count FROM worker_queue")
            .fetch_one(&self.pool)
            .await
            .map_err(|e| format!("Failed to get queue depths: {}", e))?
            .count as usize;
        
        Ok(QueueDepths {
            orchestrator_queue: orch,
            worker_queue: worker,
            timer_queue: 0,  // Timers go to orchestrator queue with delayed visibility
        })
    }
}
```

### Client Auto-Discovery

When you implement `ManagementCapability`, the Client automatically discovers it:

```rust
use duroxide::Client;

let client = Client::new(provider);

// Check if management features are available
if client.has_management_capability() {
    // Use management APIs
    let instances = client.list_all_instances().await?;
    let metrics = client.get_system_metrics().await?;
    let info = client.get_instance_info("my-instance").await?;
} else {
    println!("Provider doesn't support management features");
}
```

### Return Types

```rust
pub struct InstanceInfo {
    pub instance_id: String,
    pub orchestration_name: String,
    pub orchestration_version: String,
    pub current_execution_id: u64,
    pub status: String,              // "Running", "Completed", "Failed"
    pub output: Option<String>,      // Final output/error
    pub created_at: String,          // Timestamp
}

pub struct ExecutionInfo {
    pub instance_id: String,
    pub execution_id: u64,
    pub status: String,
    pub output: Option<String>,
    pub started_at: String,
    pub completed_at: Option<String>,
}

pub struct SystemMetrics {
    pub total_instances: usize,
    pub total_executions: usize,
    pub running_instances: usize,
    pub completed_instances: usize,
    pub failed_instances: usize,
    pub total_events: usize,
}

pub struct QueueDepths {
    pub orchestrator_queue: usize,
    pub worker_queue: usize,
    pub timer_queue: usize,  // Usually 0 (timers in orch queue)
}
```

### Performance Tips

**Efficient Status Queries:**
```sql
-- Use JOIN to get current execution status
SELECT i.instance_id, e.status
FROM instances i
JOIN executions e ON i.instance_id = e.instance_id 
  AND i.current_execution_id = e.execution_id
WHERE e.status = 'Running'
```

**Cached Metrics:**
```rust
// For high-traffic systems, consider caching metrics
struct MyProvider {
    pool: SqlitePool,
    metrics_cache: Arc<RwLock<Option<(SystemMetrics, Instant)>>>,
}

async fn get_system_metrics(&self) -> Result<SystemMetrics, String> {
    // Check cache (refresh every 30 seconds)
    if let Some((metrics, cached_at)) = &*self.metrics_cache.read().await {
        if cached_at.elapsed() < Duration::from_secs(30) {
            return Ok(metrics.clone());
        }
    }
    
    // Compute fresh metrics
    let metrics = self.compute_metrics().await?;
    *self.metrics_cache.write().await = Some((metrics.clone(), Instant::now()));
    Ok(metrics)
}
```

### Default Implementations

If you don't implement `ManagementCapability`, clients will get:
- `list_instances()` ‚Üí Empty list
- `get_system_metrics()` ‚Üí Default/zero metrics
- `has_management_capability()` ‚Üí `false`

Core orchestration execution still works normally.

---

## Testing Your Provider

After implementing your provider, follow the **[Provider Testing Guide](provider-testing-guide.md)** to:
- Run stress tests against your implementation
- Validate correctness and performance
- Compare metrics with built-in providers
- Integrate tests into your CI/CD pipeline

---

## Getting Help

- **Reference Implementation**: `src/providers/sqlite.rs`
- **Interface Definition**: `src/providers/mod.rs` (this file, with full docs)
- **Provider Tests**: `tests/sqlite_provider_test.rs`
- **Stress Tests**: `stress-tests/src/lib.rs`
- **Schema**: `migrations/20240101000000_initial_schema.sql`
- **Testing Guide**: `docs/provider-testing-guide.md`

---

**With these annotations, an LLM can implement a fully functional Provider!** üéâ

